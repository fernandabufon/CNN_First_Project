# -*- coding: utf-8 -*-
"""FirstImageClassificationProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4mgoxqgTa5pTf8rdsglWaCiQzO0HmGA

# Image Classification

**Note:** This code was developed based on a tutorial from Code Basics on YouTube ([watch tutorial](https://www.youtube.com/watch?v=7HPwo4wnJeA&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=24&ab_channel=codebasics)). It was created for educational purposes. All comments and observations are my own, reflecting my learning process and understanding of the problem and its solution.
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models # Keras is a high-level API used for building and training deep learning models
import matplotlib.pyplot as plt
import numpy as np

(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data() # cifar10 is the name of the dataset

X_train.shape # (samples, h, w, rgb channels)

X_train[0]

"""Using "X_train[i]" we will receive an array that represents one image.

**Why So Many Vectors?:** Each "vector" or set of three numbers is just showing the color of one pixel. An image in CIFAR-10 is made up of 32x32 pixels, so there are 1,024 pixels in total. Each pixel needs three numbers to describe its color, hence the many sets of three numbers.

**Understanding the Shape:** When you access X_train[0], it shows you a single image, which is composed of 32 rows of 32 pixels, and each pixel has 3 values to define its color.

***Here's a more visual way to think about it:***
**Image:** One entire array from X_train[0].
**Rows of Pixels:** Each sub-array of size 32 you see when you expand X_train[0].
**Pixels:** Each smaller array of size 3 inside those sub-arrays.
**Color Values:** The three numbers inside each of these smallest arrays.
So, even though it looks complex, all we are seeing is a detailed, color-specific representation of just one 32x32 pixel image!

*Beginning of the row*
        [[ 59,  62,  63],
        [ 43,  46,  45],
        [ 50,  48,  43],
        ...,
        [158, 132, 108],
        [152, 125, 102],
        [148, 124, 103]],
*End of the row*

Each row has 32 pixels. Each pixel has 3 values (RGB).
"""

plt.imshow(X_train[1]) # shows the image with plt

"""Now, we are going to reshape the y_train to transform it in one list (without the reshape, it is a list of arrays."""

y_train[:5]

y_train = y_train.reshape(-1,) # I write -1 as the first parameter because I don't want to change the number of samples.
y_train

"""Now I have an array that gives me a number from 0 to 9 depending on the class of the image. But I don't want to print the numbers, I want to print the name of the label."""

classes = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]

def plot_img(X, y, index): # X, y is the X_train or X_test (same with the y) and index is the sample I want to print
    plt.figure(figsize = (15,2))
    plt.imshow(X[index])
    plt.xlabel(classes[y[index]]) # the index of the class will be nthe number of the label

plot_img(X_train, y_train, 9)

"""Now it's time to normalize the data! We need to divide the values of each pixel by 255, so we will have values from 0 to 1 (instead of 0 to 255)."""

X_train = X_train/255
X_test = X_test/255

ann = models.Sequential([
    layers.Flatten(input_shape=(32,32,3)),
    layers.Dense(3000, activation='relu'),
    layers.Dense(1000, activation='relu'),
    layers.Dense(10, activation='sigmoid') # I have numbers of 0 to 9
])

ann.compile(optimizer='SGD',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
            )

ann.fit(X_train, y_train, epochs=5)

ann.evaluate(X_test, y_test)

"""Now, lets use `classification_report` to see the metrics."""

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
y_pred = ann.predict(X_test)
y_pred_classes = [np.argmax(element) for element in y_pred] # Uses "np.argmax" to find the index of the highest score for each prediction. Element -> an array with the results.

print(classification_report(y_test, y_pred_classes))

"""As we see, the accuracy is very very low (0.45). Let's implement a cnn and see what changes!"""

cnn = models.Sequential([
    # convolution neural network
    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)), # Can detect 32 features
    layers.MaxPooling2D(2,2), # Max, average...

    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    # dense network
    layers.Flatten(),
    layers.Dense(64, activation='relu'), # I don't need so many neurons and so many layers now that I'm using cnn.
    layers.Dense(10, activation='softmax') # Returns me a array of probabilities.
])

"""Note: Softmax is a function used convert a set of numbers into probabilities, which are easier to handle in classification tasks. It does this by first taking the exponential of each value to ensure all numbers are positive and to exaggerate differences, and then normalizing these values so that their total equals one. This results in a vector where each element represents the probability of a corresponding category, making it particularly useful in scenarios where a model needs to determine the likelihood of multiple different outcomes."""

cnn.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

cnn.fit(X_train, y_train, epochs=10)

"""If you compare with ANN, you can se that CNN have a better accuracy and a smaller loss."""

cnn.evaluate(X_test, y_test)

y_pred_c = cnn.predict(X_test)
y_pred_classes_c = [np.argmax(element) for element in y_pred_c]

print(classification_report(y_test, y_pred_classes_c))

"""Precision: It is the ratio of correct predictions for a class among all predictions made for that class.

Recall: Represents the proportion of actual cases of a class that were correctly identified by the model.

F1-Score: It is a harmonic mean between precision and recall, providing a single number that balances these two metrics.

Support: Refers to the number of occurrences of each actual class in the dataset.

# Exercise:

Use CNN to do handwritten digits classification using MNIST dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

"""Getting the dataset from `keras.datasets`"""

(X_train, y_train) , (X_test, y_test) = keras.datasets.mnist.load_data()

y_train # notice that in the image datasets, the "y" was an array of arrays. Now, it is only an array.

X_train[0].shape # We have 28x28 pixels in each image.

plt.matshow(X_train[0])

"""Now let's normalize the data."""

X_train = X_train / 255
X_test = X_test / 255

"""Let's build the model without using cnn."""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)), # This transforms the 2D 28x28 image into a 1D array of 784 elements
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

model.evaluate(X_test,y_test)

"""Now, let's aply some cnn layers in this model:"""

y_train.shape

model = keras.Sequential([
    keras.layers.Conv2D(filters=56, kernel_size=(4,4), activation='relu', input_shape=(28,28,1)),
    layers.MaxPooling2D(2,2),
    keras.layers.Conv2D(filters=16, kernel_size=(4,4), activation='relu'),
    layers.MaxPooling2D(2,2),

    keras.layers.Flatten(), # This transforms the 2D 28x28 image into a 1D array of 784 elements
    keras.layers.Dense(100, activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10)

model.evaluate(X_test,y_test)

y_pred = model.predict(X_test)
y_pred = [np.argmax(element) for element in y_pred]

print(classification_report(y_test, y_pred))

